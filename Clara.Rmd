---
title: "Clara"
author: "Anqi Chi SID:460204008"

date: "22/10/2018"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
library(knitr)
library(kableExtra)
```


```{r echo=FALSE}
Data <- read.csv("cleanedData.csv")
adultData<-Data[Data$AGEC>=18,]
obese<-factor(ifelse(adultData$BMISC>=30,'1','0'))
Xy =na.omit(cbind(adultData[,c("BDYMSQ04","BMR","SEX","EIBMR1","ADTOTSE","CHOPER1","FATPER1","PROPER1")],obese))
X<-Xy[,-ncol(Xy)]
y<-Xy[,ncol(Xy)]
X$BDYMSQ04=as.numeric(ifelse(X$BDYMSQ04 == 5, '0', '1'))
```

# LDA
```{r,echo=FALSE}
# Load the functions from cvTools into memory.
# First we are going to load the cross-validation function
cvFolds = function (n, K = 5, R = 1, type = c("random", "consecutive", "interleaved")) 
{
  n <- round(rep(n, length.out = 1))
  if (!isTRUE(n > 0)) 
    stop("'n' must be positive")

  K <- round(rep(K, length.out = 1))
  if (!isTRUE((K > 1) && K <= n)) 
    stop("'K' outside allowable range")

  type <- if (K == n) 
    "leave-one-out"
  else match.arg(type)
  if (type == "random") {
    R <- round(rep(R, length.out = 1))
    if (!isTRUE(R > 0)) 
      R <- 1
    subsets <- replicate(R, sample(n))
  }
  else {
    R <- 1
    subsets <- as.matrix(seq_len(n))
  }
  which <- rep(seq_len(K), length.out = n)
  if (type == "consecutive") 
    which <- rep.int(seq_len(K), tabulate(which))
  folds <- list(n = n, K = K, R = R, subsets = subsets, which = which)
  class(folds) <- "cvFolds"
  folds
}
```

```{r,echo=FALSE}
library(MASS)
cvFolds = function (n, K = 5, R = 1, type = c("random", "consecutive", "interleaved")) 
{
  n <- round(rep(n, length.out = 1))
  if (!isTRUE(n > 0)) 
    stop("'n' must be positive")

  K <- round(rep(K, length.out = 1))
  if (!isTRUE((K > 1) && K <= n)) 
    stop("'K' outside allowable range")

  type <- if (K == n) 
    "leave-one-out"
  else match.arg(type)
  if (type == "random") {
    R <- round(rep(R, length.out = 1))
    if (!isTRUE(R > 0)) 
      R <- 1
    subsets <- replicate(R, sample(n))
  }
  else {
    R <- 1
    subsets <- as.matrix(seq_len(n))
  }
  which <- rep(seq_len(K), length.out = n)
  if (type == "consecutive") 
    which <- rep.int(seq_len(K), tabulate(which))
  folds <- list(n = n, K = K, R = R, subsets = subsets, which = which)
  class(folds) <- "cvFolds"
  folds
}
# k - value of k in KNN
# 780a6b9a975525f57598113d0798946254bf476c
V =10 #number of CV folds
# seed - (optional) internally sets the seed.
cv.da = function(X,y,method=c("lda","qda"),V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error.da <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
    
    # Do classification on ith fold
    if (method=="lda") {
      res <- lda(y~., data=X,subset=trainInds)
    }
    if (method=="qda") {
      res <- qda(y~., data=X,subset=trainInds)
    }
    results.da = predict(res, X.test)$class
    
    # Calcuate the test error for this fold
    test.error.da[i] <- sum(results.da!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error.da)/n
  
  # Return the results
  return(cv.error)
}
```


```{r,echo=FALSE}
n = length(y)
dat = data.frame(y,X)
X1 = model.matrix(~-1+BMR+EIBMR1+ADTOTSE+SEX+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
X1 = data.frame(X1)
res <- lda(y~., data=X1,subset=1:n)
## Kable
lda.table<-round(coef(res),6)
rownames(lda.table)<-c('BMR','Energy Intake','Total mins spent sedentary','Sex','Whether currently on a diet','Carbohydrate diet','Fat diet','Protein diet')
colnames(lda.table)<-c('Estimated Coefficients')
kable_styling(kable(
  lda.table,
  booktabs=TRUE,
  caption = "Coefficients of Predictors in LDA"
  ),
  latex_options = 'hold_position',
  position = "center")
```
##Comment:

The above output suggests the following interpretations for each of the variables.

* People who have higher BMR are more likely to be obese.
* Lower EIBMR1(Energy intake) increases obesity probability (since the coefficient -0.434 is negative).
* Lower ADTOTSE(Total mins spent sitting or lying down) increases obesity probability.
* Sex = 1 for males. So females are more likely to be obese.
* People on a diet are more likely to be obese compared with people not on a diet
* People who have high fat or high protein diet type are more likely to be obese
* People who have high carbon diet type decrease the probability of obesity

```{r,echo=FALSE}
#cross validation error for LDA.
res.lda = cv.da(X1,y,method="lda",V,seed=1)
```

# CART
```{r,echo=FALSE}
library(rpart)
X2 = model.matrix(~-1+BMR+EIBMR1+ADTOTSE+SEX+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
# Be careful as coding y as a factor here, Otherwise R will do a regression tree rather than a classification tree
res.rpart <- rpart(as.factor(y) ~ X2, data=dat)

library(rpart.plot)
rpart.plot(res.rpart,type=0,extra=1,main="CART Fit for Obesity ",cex.main=2,cex=1)

#male table
male.props<-c(' 6.7%',' 68.5%')
male.df<-t(data.frame(male=male.props))
colnames(male.df)<-c('6103 < BMR < 8297','BMR > 8297')
rownames(male.df)<-c('estimated obesity rate')
kable_styling(kable(
  male.df,
  booktabs=TRUE,
  caption = "Obesity Rate for Male with BMR greater than 6103"
  ),
  latex_options = 'hold_position',
  position = "center")

# female
female.props<-c('45.8%','80.1%')
female.df<-t(data.frame(female=female.props))
colnames(female.df)<-c('6103 < BMR < 6390','BMR > 6390')
rownames(female.df)<-c('estimated obesity rate')
kable_styling(kable(
  female.df,
  booktabs=TRUE,
  caption = "Obesity Rate for Female with BMR greater than 6103"
  ),
  latex_options = 'hold_position',
  position = "center")
```
##Comment:

* 91.4 percent of people(including male and female) whose BMR are less than 6103 are normal (not obese).
* 83.3 percent of male whose BMR are between 6103 and 8297 are normal.
* 68.5 percent of male whose BMR are greater than 8297 are obese.
* 54.2 percent of female whose BMR are between 6103 and 6390 are normal.
* 80.1 percent of female whose BMR are greater than 6390 are obese.


```{r,echo=FALSE}
#cross-validation for rpart.
cv.rpart = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.rpart <- rpart(as.factor(y.train) ~., data=X.train)
    res = predict(res.rpart, newdata=X.test, type = "class")
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

```{r,echo=FALSE}
res.rpart = cv.rpart(X2,y,V,seed=1)
```


# Logistic regression
```{r,echo=FALSE}
#Fit a logistic regression model on most of the data.
res.glm = glm(y~.,family=binomial,data=X1)
coef.table<-round(coef(res.glm),6)
coef.table1<-cbind(c('1',paste('$x_',1:8,'$',sep = '')),coef.table)
colnames(coef.table1)<-c('Variable','Estimated Coefficient')
kable(coef.table1,escape = FALSE,caption = 'Estimated Coefficients of the Logistic Regression') %>%
kable_styling(latex_options='hold_position',position='center')
#coef.table1
```
##Comment:
The full model has the coefficeints for BMR, EIBMR1, ADTOTSE, SEX, BDYMSQ04, CHOPER1 as statisticlly
signifficantly different from zero at the 0.05 level. The fitted model is
$$
\begin{split}
\mbox{logit}(p) = &-17.770903  + 0.001879 x_1 - 0.711322 x_2 - 0.00006 x_3 + 3.422641 x4  \\
& + 0.380154 x_5 - 0.010135 x_6 + 0.001507 x_7 â€“ 0.000998 x_8 
\end{split}
$$
where $p$ is the probability of obesity.

where $x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8$ represents BMR, EIBMR1, ADTOTSE, SEX, BDYMSQ04, CHOPER1, FATPER1, PROPER1 respectively.

```{r,echo=FALSE}
cv.glm = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.glm <- glm(y.train ~., data=X.train, family=binomial)
    res = round(predict(res.glm, newdata=X.test, type="response"))
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

```{r echo=FALSE}
res.glm.cv = cv.glm(X1,y,V,seed=1)
```

#summary
```{r,echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(huxtable)
library(kableExtra)
#Kable
Methods = c( 'LDA', 'CART', 'Logistic Regression')
Errors = round(100*c(res.lda, res.rpart, res.glm.cv),3)
errors.df<-(data.frame(Methods,Errors))
kable_styling(kable(
  errors.df,
  booktabs=TRUE,
  caption = "Summary of the CV Errors"
  ),
  latex_options = 'hold_position',
  position = "center")
```
The CV error for LDA is 18.232 percent. The CV error for CART is 17.551 percent. The CV error for glm is 17.778 percent