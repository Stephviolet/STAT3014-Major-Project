---
title: "Clara"
author: "Anqi Chi SID:460204008"
date: "2018 October"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
```

#Data Clean
```{r echo=FALSE}
Data <- read.csv("cleanedData.csv")
dim(Data)
adultData<-Data[Data$AGEC>=18,]
# low.fat_high.carb<-Data$FATPER1<25 & Data$CHOPER1>55
# high.protein_low.carb<-Data$PROPER1>20 & Data$CHOPER1<45
# high.fat_low.carb<-Data$FATPER1>30 & Data$CHOPER1<45
carb.cat<-cut(adultData$CHOPER1,
              breaks=c(-1,45,65,100),
              labels=c('low','medium','high'))
fat.cat<-cut(adultData$FATPER1,
             breaks=c(-1,20,35,100),
             labels=c('low','medium','high'))
protein.cat<-cut(adultData$PROPER1,
                 breaks=c(-1,15,25,100),
                 labels=c('low','medium','high'))

#data<-na.omit(data.frame(BMR=DataC$BMR,Sex=sex,Diet=ifDiet,EIBMR=DataC$EIBMR1))
obese<-factor(ifelse(adultData$BMISC>=30,'1','0'))
Xy =na.omit(cbind(adultData[,c("BDYMSQ04","BMR","SEX","EIBMR1","ADTOTSE","CHOPER1","FATPER1","PROPER1")],obese))
X<-Xy[,-ncol(Xy)]
y<-Xy[,ncol(Xy)]
#Code the categorical variables as factors.
X$SEX = as.numeric(X$SEX=="1")
#sex<-factor(ifelse(DataC$SEX==1,'male','female'))
X$BDYMSQ04=as.numeric(ifelse(X$BDYMSQ04 == 5, '0', '1'))
summary(X)
```

# LDA

```{r,echo=FALSE}
library(MASS)
cvFolds = function (n, K = 5, R = 1, type = c("random", "consecutive", "interleaved")) 
{
  n <- round(rep(n, length.out = 1))
  if (!isTRUE(n > 0)) 
    stop("'n' must be positive")

  K <- round(rep(K, length.out = 1))
  if (!isTRUE((K > 1) && K <= n)) 
    stop("'K' outside allowable range")

  type <- if (K == n) 
    "leave-one-out"
  else match.arg(type)
  if (type == "random") {
    R <- round(rep(R, length.out = 1))
    if (!isTRUE(R > 0)) 
      R <- 1
    subsets <- replicate(R, sample(n))
  }
  else {
    R <- 1
    subsets <- as.matrix(seq_len(n))
  }
  which <- rep(seq_len(K), length.out = n)
  if (type == "consecutive") 
    which <- rep.int(seq_len(K), tabulate(which))
  folds <- list(n = n, K = K, R = R, subsets = subsets, which = which)
  class(folds) <- "cvFolds"
  folds
}
# k - value of k in KNN
V =10 #number of CV folds
# seed - (optional) internally sets the seed.
cv.da = function(X,y,method=c("lda","qda"),V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error.da <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
    
    # Do classification on ith fold
    if (method=="lda") {
      res <- lda(y~., data=X,subset=trainInds)
    }
    if (method=="qda") {
      res <- qda(y~., data=X,subset=trainInds)
    }
    results.da = predict(res, X.test)$class
    
    # Calcuate the test error for this fold
    test.error.da[i] <- sum(results.da!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error.da)/n
  
  # Return the results
  return(cv.error)
}
```

Fit the LDA method
```{r,echo=TRUE}
n = length(y)
dat = data.frame(y,X)
X1 = model.matrix(~-1+BMR+SEX+EIBMR1+ADTOTSE+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
X1 = data.frame(X1)
summary(X1)
res <- lda(y~., data=X1,subset=1:n)
res

```
##Comment:
The above output suggests the following interpretations for each of the variables.
* People who have higher BMR are more likely to obese
* Sex = 1 for males. So females are more likely to obese
* Lower EIBMR1(Energy intake) increases obese probability (since the coefficent -0.377 is negative).
* Lower ADTOTSE(Total mins spent sitting or lying down) increases obese probability (since the coefficent -3.533708e-05 is negative).
* People who have high fat and high carbon diet type are less likely to obese
* People who have high protein diet type increase the probability of obesity


##CV error
```{r,echo=TRUE}
#cross validation error for LDA.
res.lda = cv.da(X1,y,method="lda",V,seed=1)
res.lda 
```
Comment:
The CV error for LDA is 18.232 percent

# CART

```{r,echo=TRUE}
library(rpart)
X2 = model.matrix(~-1+BMR+SEX+EIBMR1+ADTOTSE+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
# Be careful as coding y as a factor here, Otherwise R will do a regression tree rather than a classification tree
res.rpart <- rpart(as.factor(y) ~ X2, data=dat)

library(rpart.plot)
rpart.plot(res.rpart,type=0,extra=1,main="CART fit",cex.main=2,cex=1)
```
##Comment:
Interpret

cross-validation for rpart.
```{r,echo=FALSE}
cv.rpart = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.rpart <- rpart(as.factor(y.train) ~., data=X.train)
    res = predict(res.rpart, newdata=X.test, type = "class")
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

##CV error
```{r,echo=TRUE}
res.rpart = cv.rpart(X2,y,V,seed=1)
res.rpart
```
Comment:
The CV error for CART is 17.551 percent

# Logistic regression

Fit a logistic regression model on most of the data.
```{r,echo=TRUE}
res.glm = glm(y~.,family=binomial,data=X1)
summary(res.glm)
```
##Comment:
The full model has the coefficeints for BMR, SEX, EIBMR1, ADTOTSE, BDYMSQ04, CHOPER1 as statisticlly
signifficantly different from zero at the 0.05 level. Te fitted model is
$$
\mbox{logit}(p) = -0.1093 + 0.001879 \cdot\mbox{BMR} - 3.423 \cdot \mbox{SEX} - 0.7113 \cdot\mbox{EIBMR1} -0.00005952 \cdot \mbox{ADTOTSE} + 0.3802 \cdot \mbox{BDYMSQ04} - 0.01014 \cdot \mbox{CHOPER1} 
$$
where $p$ is the probability of obesity.

The effect of the significant variables on surival are:
* Larger BMR reduces the probability of obesity.
* Males have reduced the probability of obesity compared to women.



```{r,echo=FALSE}
cv.glm = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.glm <- glm(y.train ~., data=X.train, family=binomial)
    res = round(predict(res.glm, newdata=X.test, type="response"))
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

###CV error for glms.
```{r echo=FALSE}
res.glm.cv = cv.glm(X1,y,V,seed=1)
res.glm.cv
```
Comment:
The CV error for glm is 17.78 percent

#summary
```{r,echo=FALSE}
library(huxtable)
library(kableExtra)
ht <- hux(
        Methods = c( 'LDA', 'rpart', 'glm'),
        Errors = 100*c(res.lda, res.rpart, res.glm.cv), 
        add_colnames = TRUE
      )

bold(ht)[1,]           <- TRUE
bottom_border(ht)[1,]  <- 1
align(ht)[,2]          <- 'right'
right_padding(ht)      <- 10
left_padding(ht)       <- 10
width(ht)              <- 0.35

#ht
Methods = c( 'LDA', 'rpart', 'glm')
Errors = 100*c(res.lda, res.rpart, res.glm.cv)
kable(data.frame(Methods,Errors))

```

The effect of different predictors on obesity are relatively
consistent. In summary:
ginger