---
title: "Clara"
author: "Anqi Chi SID:460204008"

date: "22/10/2018"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(tidyverse)
library(knitr)
library(kableExtra)
```

```{r echo=FALSE}
Data <- read.csv("cleanedData.csv")
adultData<-Data[Data$AGEC>=18,]
obese<-factor(ifelse(adultData$BMISC>=30,'1','0'))
Xy =na.omit(cbind(adultData[,c("BDYMSQ04","BMR","SEX","EIBMR1","ADTOTSE","CHOPER1","FATPER1","PROPER1")],obese))
X<-Xy[,-ncol(Xy)]
y<-Xy[,ncol(Xy)]
X$BDYMSQ04=as.numeric(ifelse(X$BDYMSQ04 == 5, '0', '1'))
```
To predict the obesity (level of BMI is greater than 30), we chose eight variables as the predictors, which include BMR, energy intake, total minutes spent sitting or lying down, sex, whether currently on a diet, percentage of energy from carbohydrate, percentage of energy from total fat and percentage of energy from protein. 
We applied linear discriminant analysis, classification and regression tree and logistic regression to investigate our question and estimated using 10-fold cross-validation.

We assign the level of BMI greater than 30 as number 1 which represent obesity and the remaining as number 0 to represent normal people. we also assign people currently on a diet as number 1 and those not a on a diet as number 0, and ignore the situation not known if currently on a diet and not applicable. Thus, we treat all variables that we are interested as numerical variables so that we increase the effectiveness of our analysis and the interpretability of our results.


# LDA
For our prediction attempt, we first use linear discriminant analysis rather than quadratic discriminant analysis because the Interpretation for QDA  is more difficult, but the group means suggest identical interpretations as for LDA.
After fitting the LDA method, the output (Table \ref{lda_table}) indicates the following interpretations for each of the variables. People who have higher BMR are more likely to be obese as the coefficient of linear discriminant 0.001249 is positive. Lower energy intake increases obesity probability since the coefficient of linear discriminant -0.434204 is negative. People who have lower total minutes spent sedentary are prone to be obese. Females are more likely to have obesity and people on a diet are more likely to be obese compared with people not on a diet. People who have high fat or high protein diet type are more likely to be obese while the probability of obesity reduces with high carbohydrate diet type.

```{r,echo=FALSE}
# Load the functions from cvTools into memory.
# First we are going to load the cross-validation function
cvFolds = function (n, K = 5, R = 1, type = c("random", "consecutive", "interleaved")) 
{
  n <- round(rep(n, length.out = 1))
  if (!isTRUE(n > 0)) 
    stop("'n' must be positive")

  K <- round(rep(K, length.out = 1))
  if (!isTRUE((K > 1) && K <= n)) 
    stop("'K' outside allowable range")

  type <- if (K == n) 
    "leave-one-out"
  else match.arg(type)
  if (type == "random") {
    R <- round(rep(R, length.out = 1))
    if (!isTRUE(R > 0)) 
      R <- 1
    subsets <- replicate(R, sample(n))
  }
  else {
    R <- 1
    subsets <- as.matrix(seq_len(n))
  }
  which <- rep(seq_len(K), length.out = n)
  if (type == "consecutive") 
    which <- rep.int(seq_len(K), tabulate(which))
  folds <- list(n = n, K = K, R = R, subsets = subsets, which = which)
  class(folds) <- "cvFolds"
  folds
}
```

```{r,echo=FALSE}
library(MASS)
cvFolds = function (n, K = 5, R = 1, type = c("random", "consecutive", "interleaved")) 
{
  n <- round(rep(n, length.out = 1))
  if (!isTRUE(n > 0)) 
    stop("'n' must be positive")

  K <- round(rep(K, length.out = 1))
  if (!isTRUE((K > 1) && K <= n)) 
    stop("'K' outside allowable range")

  type <- if (K == n) 
    "leave-one-out"
  else match.arg(type)
  if (type == "random") {
    R <- round(rep(R, length.out = 1))
    if (!isTRUE(R > 0)) 
      R <- 1
    subsets <- replicate(R, sample(n))
  }
  else {
    R <- 1
    subsets <- as.matrix(seq_len(n))
  }
  which <- rep(seq_len(K), length.out = n)
  if (type == "consecutive") 
    which <- rep.int(seq_len(K), tabulate(which))
  folds <- list(n = n, K = K, R = R, subsets = subsets, which = which)
  class(folds) <- "cvFolds"
  folds
}
# k - value of k in KNN
# 780a6b9a975525f57598113d0798946254bf476c
V =10 #number of CV folds
# seed - (optional) internally sets the seed.
cv.da = function(X,y,method=c("lda","qda"),V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error.da <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
    
    # Do classification on ith fold
    if (method=="lda") {
      res <- lda(y~., data=X,subset=trainInds)
    }
    if (method=="qda") {
      res <- qda(y~., data=X,subset=trainInds)
    }
    results.da = predict(res, X.test)$class
    
    # Calcuate the test error for this fold
    test.error.da[i] <- sum(results.da!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error.da)/n
  
  # Return the results
  return(cv.error)
}
```

```{r,echo=FALSE}
n = length(y)
dat = data.frame(y,X)
X1 = model.matrix(~-1+BMR+EIBMR1+ADTOTSE+SEX+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
X1 = data.frame(X1)
res <- lda(y~., data=X1,subset=1:n)
## Kable
lda.table<-round(coef(res),6)
rownames(lda.table)<-c('BMR','Energy Intake','Total mins spent sedentary','Sex','Whether currently on a diet','Carbohydrate diet','Fat diet','Protein diet')
colnames(lda.table)<-c('Estimated Coefficients')
kable_styling(kable(
  lda.table,
  booktabs=TRUE,
  caption = "\\label{lda_table}Coefficients of Predictors in LDA"
  ),
  latex_options = 'hold_position',
  position = "center")
```


```{r,echo=FALSE}
#cross validation error for LDA.
res.lda = cv.da(X1,y,method="lda",V,seed=1)
```

# CART
In addition, we proceeded to fit a CART for obesity by the dependent variables we selected before and then we got the output below of CART Fit for Obesity. The tree could be explained precisely by Table \ref{cart_male} and Table \ref{cart_female}. We found that there is no difference of obesity rate between male and female whose level of BMR are lower than 6103. The obesity rate is 68.5 percent for male with the level of BMR greater than 8297 while 80.1 percent of female with BMR greater than 6103 are obese. 83.3 percent of male whose BMR are between 6103 and 8297 are normal. 54.2 percent of female whose BMR are between 6103 and 6390 are normal. Based on this analysis, we may conclude that at the same level of BMR, female are more likely to have obesity than male.

```{r,echo=FALSE}
library(rpart)
X2 = model.matrix(~-1+BMR+EIBMR1+ADTOTSE+SEX+BDYMSQ04+CHOPER1+FATPER1+PROPER1,data=dat)
# Be careful as coding y as a factor here, Otherwise R will do a regression tree rather than a classification tree
res.rpart <- rpart(as.factor(y) ~ X2, data=dat)

library(rpart.plot)
```

```{r,echo=FALSE, fig.cap="\\label{cart_fig}CART Fit for Obesity"}
rpart.plot(res.rpart,type=0,extra=1,main='CART Fit for Obesity',cex.main=1,cex=0.6)
```

```{r, echo=FALSE}
#male table
male.props<-c(' 6.7%',' 68.5%')
male.df<-t(data.frame(male=male.props))
colnames(male.df)<-c('6103 < BMR < 8297','BMR > 8297')
rownames(male.df)<-c('estimated obesity rate')
kable_styling(kable(
  male.df,
  booktabs=TRUE,
  caption = "\\label{cart_male}Obesity Rate for Male with BMR greater than 6103"
  ),
  latex_options = 'hold_position',
  position = "center")

# female
female.props<-c('45.8%','80.1%')
female.df<-t(data.frame(female=female.props))
colnames(female.df)<-c('6103 < BMR < 6390','BMR > 6390')
rownames(female.df)<-c('estimated obesity rate')
kable_styling(kable(
  female.df,
  booktabs=TRUE,
  caption = "\\label{cart_female}Obesity Rate for Female with BMR greater than 6103"
  ),
  latex_options = 'hold_position',
  position = "center")
```


```{r,echo=FALSE}
#cross-validation for rpart.
cv.rpart = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.rpart <- rpart(as.factor(y.train) ~., data=X.train)
    res = predict(res.rpart, newdata=X.test, type = "class")
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

```{r,echo=FALSE}
res.rpart = cv.rpart(X2,y,V,seed=1)
```


# Logistic regression
As we are interested in a special case on a generalised linear model for binary data which is obesity in our case, we fit a logistic regression model on the same data. The full model has the coefficients for BMR, energy intake, total minutes spent sedentary, sex, whether currently on a diet, percentage of energy from carbohydrate as statistically significantly different from zero at the 0.05 level. Table \ref{logistic} shows the Estimated Coefficients of the Logistic Regression.

```{r,echo=FALSE}
#Fit a logistic regression model on most of the data.
res.glm = glm(y~.,family=binomial,data=X1)
coef.table<-round(coef(res.glm),6)
coef.table1<-cbind(c('1',paste('$x_',1:8,'$',sep = '')),coef.table)
colnames(coef.table1)<-c('Variable','Estimated Coefficient')
kable(coef.table1,escape = FALSE,caption = '\\label{logistic}Estimated Coefficients of the Logistic Regression') %>%
kable_styling(latex_options='hold_position',position='center')
#coef.table1
```

```{r,echo=FALSE}
cv.glm = function(X,y,V,seed=NA)
{
  # Set the seed
  if (!is.na(seed)) {
    set.seed(seed)
  }
  
  # Set n
  n = length(y)
  
  # Split the data up into V folds
  cvSets <- cvFolds(n, V)
  
  # Loop through each fold and calculate the error for that fold
  test.error <- c()
  for (i in 1:V) 
  {
    # set the indices corresponding to the training and test sets
    testInds <- cvSets$subsets[which(cvSets$which==i)]
    trainInds <- (1:n)[-testInds]
    
    X = data.frame(X)
    
    # Separate y and X into the training and test sets
    y.test <- y[testInds]
    X.test <- X[ testInds,]
    y.train <- y[trainInds]
    X.train <- X[trainInds,]
  
    # Do classification on ith fold
    res.glm <- glm(y.train ~., data=X.train, family=binomial)
    res = round(predict(res.glm, newdata=X.test, type="response"))
    
    # Calcuate the test error for this fold
    test.error[i] <- sum(res!=y.test)
  }
  
  # Calculate the mean error over each fold
  cv.error = sum(test.error)/n
  
  # Return the results
  return(cv.error)
}
```

```{r echo=FALSE}
res.glm.cv = cv.glm(X1,y,V,seed=1)
```

The fitted model is
$$
\begin{split}
\mbox{logit}(p) = &-17.770903  + 0.001879 x_1 - 0.711322 x_2 - 0.00006 x_3 + 3.422641 x4  \\
& + 0.380154 x_5 - 0.010135 x_6 + 0.001507 x_7 - 0.000998 x_8 
\end{split}
$$
where $p$ represent the probability of obesity.

where $x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8$ represents BMR, EIBMR1, ADTOTSE, SEX, BDYMSQ04, CHOPER1, FATPER1, PROPER1 respectively.

From this fitted model, we noticed that the effects of the significant variables on obesity are the same with LDA,  and we can get similar explanations with LDA method but differs that high percentage of energy from protein are less likely to be obese.

#summary

```{r,echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(huxtable)
library(kableExtra)
#Kable
Methods = c( 'LDA', 'CART', 'Logistic Regression')
Errors = round(100*c(res.lda, res.rpart, res.glm.cv),3)
errors.df<-(data.frame(Methods,Errors))
kable_styling(kable(
  errors.df,
  booktabs=TRUE,
  caption = "\\label{errors_summary}Summary of the CV Errors"
  ),
  latex_options = 'hold_position',
  position = "center")
```
Finally, we compare these three methods by computing repeated 10-fold cross-validation errors. Table \ref{errors_summary} shows that the CV error for CART is 17.551 percent which is better than LDA and Logistic Regression. The effect of different methods on obesity are relatively consistent.